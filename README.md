## united.cloud-devops-test

For the purpose of these interview questions let's assume OS is Ubuntu Server 22.04 LTS x64. You will need GIT repo for these so I suggest you create gitlab.com or github.com private repo and commit all your work there, you can share this repo with sibin.arsenijevic@united.cloud, vladimir.rusovac@united.cloud and boris.petrovic@united.cloud just be sure to add us as developers on gitlab as it allows us to view source code. 

Each of these tasks should be done on a separate branch and later merged into master.

1. If we assume running some of the services causes "Too many open files" errors in logs write Ansible (preferred) or Puppet or Chef "playbook" which sets this limit to the higher value.

2. Create simple Python script which will fetch current IP geoip info from http://www.geoplugin.net/json.gp?ip=<current_ip> API and store it into SQLite database along with all geoip info that is available through API. Each record needs to be timestamped. Script has to loop and fetch results every minute. You have to use requests library in your script. You may not install requests as python global package. Script should be PEP8 compliant (or rather pass pylint check with flying colors)

3. Package the said script into Docker container and ensure that script runs when container is started and that sqlite database is created on host machine (not inside the container). Bonus points for doing this with smallest possible docker image.

4. Given that you have complex time-critical high-traffic bare-metal java-powered infrastructure and also lighter virtualized services (cmdb, jenkins, nexus, gitlab, etc) describe your vision for monitoring, provisioning, de-provisioning, back-up-ing, hypervisor-ing, log collecting? This question is aimed more at technologies / products that you would use so giving a list of technologies and brief reason for them (1 or 2 sentences per tech is enough). (there is no "right" answer)

5. At 4 a.m. Friday night you get a call from NOC member that one of the servers on one of the locations is reported as down and that he has tried to ssh over public network to it but it just times out. Since this is one of the "more important" servers you have to check it yourself. You know that there are other servers at the same location and that they are all interconnected through a switch in a "private" network which you can access. SSH is not bound to any interface. What would be your troubleshooting steps? Explain what are you checking and why. Try to come up with the most elaborate "fail scenario". (there is no "right" answer)

6. (Bonus Question) Looking at the graphs of your monitoring solution one of your high-traffic servers has few of the cores on 100% and others running fairly low at 20%. Looking at top/htop output this load is not coming from "userland". What would your troubleshooting steps be? (there is no "right" answer)

## Solutions:

1. Take a look at README.md in `ansible` directory.

4. I decided to expand the questions just a little bit and add some recommendations about storage and filesystem as well as these are also pretty important while talking about a resilient infrastructure. 
    1. ### Storage
        Obviously we're running some sort of storage array and it's configured to use [RAID 10](https://www.diffen.com/difference/RAID-5-vs-RAID-10) (stripe of mirrors) and not [RAID 5](https://www.diffen.com/difference/RAID-5-vs-RAID-10) (uses parity disks) as RAID 5 has proven quite unreliable in enterprise environment. Losing 1 disk in RAID 5 would put so much pressure on remaining disks that it wasn't uncommon for more disks to die, leading to collapse of RAID array and data loss.
    2. ### Filesystem
        Using one of the modern Copy On Write filesystem like [ZFS or BTRFS](https://www.wundertech.net/btrfs-vs-zfs-comparison/) would be my choice while running any sort of workload in data-centers (not so much in the cloud). There is only one word that can describe these two and the word is amazing. :) One advantage of COW filesystem are lightning fast snapshot creation AND return to previous snapshot after a disaster (System unbootable after upgrade? No problem, just log into rescue and return previous snapshot that was automatically created before the upgrade). Something that is directly tied to snapshot creation is the ability for admin to copy file(s) from previous snapshots by going to hidden .zfs or .btrfs dir.
    3. ### VM Hypervisor
        I don't have a particular preference here, I've worked in a professions manner with both [VMWare ESXi](https://en.wikipedia.org/wiki/VMware_ESXi) and [KVM QEMU](https://en.wikipedia.org/wiki/QEMU), they both work very well and I have no doubt the same can be said for Oracle or MS enterprise offerings. They all offer VM snap-shotting for migration and backup purposes.
    4. ### Backups
        While we are pretty well covered with RAID, CoW FS, and VM snapshots we should add some sort of FS based backups, just in case. Simplest solution if to use cron and [rsync](https://linux.die.net/man/1/rsync), but a more robust solution like [Veeam](https://www.veeam.com/) is probably not a bad idea for enterprise use.
    5. ### Configuration management
        [Ansible](https://en.wikipedia.org/wiki/Ansible_(software)) is obviously a clear winner in this department. One (big) disadvantage of Ansible is that there is no automatic reconciliation of actual state to desired state until you actually run the playbooks, so there is no automated way to stop configuration drift. Ansible can also be used for provisioning/deprovisioning of VMs, altho it's not the best tool for that use.
    6. ### Infrastructure as Code
        [Terraform](https://www.terraform.io/) is the defacto industry standard (for cloud operations, can even be used in data-centers if you're running something like [OpenStack](https://www.openstack.org/)) at this point as it provides huge number of providers and modules. There are a few disadvantages to it, no automatic reconciliation of state and it's DSL is pretty inflexible in some cases ([take a look at what you need to do in order to loop over a nested map](https://stackoverflow.com/questions/63500554/terraform-iterate-over-nested-map)). Some of the notable contenders that might surpass it in time are [Pulumi](https://www.pulumi.com/docs/concepts/vs/terraform/), [AWS CDK](https://aws.amazon.com/cdk/) and (most interesting for me) [Crossplane](https://blog.crossplane.io/crossplane-vs-terraform/). Crossplane is, finally, a tool that solves configuration drift in an automated way!
    7. ### Logging
        [Loki](https://grafana.com/oss/loki/) all the way! I'm very impressed with how good and lightweight Loki is compared to [ELK stack](https://logz.io/learn/complete-guide-elk-stack/#what-elk-stack). Loki requires a log forwarder, default one is called [Promtail](https://grafana.com/docs/loki/latest/clients/promtail/), but it's fairly inflexible (being able to only ship logs to Loki), other option is [FluentBit](https://docs.fluentbit.io/manual/pipeline/outputs) that can be configured to have more than one output stream (e.g. send logs to Loki for operational uses, but also ship sensitive logs to some storage account for long term audit and compliance reasons).
    8. ### Monitoring
        [Prometheus](https://prometheus.io/) + [Grafana](https://grafana.com/) are the weapons of choice these days. Add [AlertManager](https://prometheus.io/docs/alerting/latest/alertmanager/) to the mix and you have a comprehensive solution for monitoring, visualizing and reporting issues to whomever needs to know about them. :) These 3 and a few more services are bundled in a very neat pack by [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack) Helm chart.

5. Any good troubleshooting run start with going to the monitoring solution of our choice to check if there are some alerts regarding the server in question. A server that is down is bound to produce a whole array of alerts. First priority is getting the server operational (if possible) while finding the root cause of the issue and corroborating that with logs/alerts for postmortem is of secondary importance.
    - If alerts are few (maybe just related to ssh connectivity) that is a good sign, at least we can conclude that the server is still operational and so is networking (as monitoring agent is still sending data). In that case problem is software related, perhaps ssh-server is down, maybe an upgrade messed up config file (or junior admin was playing with it :) ) and the service is down? If the server is VM solution is easy, log into the server in question through the hypervisor and look around, check status of ssh-server. If the server is bare metal then someone is going to take a trip to the data center. Before I start the car I would probably check the logs (since we're using centralized logging solution) for anything suspicious (correlate time of appearance of alerts in monitoring and check logs around that time). Also firewall rules could be the culprit, so double check with NOC if they've been playing with those rules.
    - If alerts are many that means the server is actually down. If it's a VM go through hypervisor and see what's going on, check if there are some boot related errors. Try previous version of kernel and see if it boots up, alternatively return known working version of VM image snapshot. If the server is bare metal someone should repeat the same steps in the DC. If it's completely nonoperational then hardware malfunction would be the most likely cause. Servers have quite a bit of hardware redundancy (double PSU, double network interfaces, connected to two different switches, two or more CPUs, several sticks of memory), but what they don't have are double motherboards. If MB died the only course of action would be to send it to the vendor for repairs.

6. A very difficult question to answer without access to the server in question, but I'll give it my best. :)
    - First step for me would be check exact time when monitoring alerts appeared (and to see if this has happened before) and then try to correlate that time with certain system level logs. Any errors or warnings at the approximate time should be further pursued by the help of Google.
    - Question states that the problem is not originating in the userland so it must be from the kernel, check `htop` too see if anything pops out regarding kernel processes and if it does pursue that lead. Correlate PID of suspicious process(es) with `ps auxwwf` for more data.
    - Are there some interrupts that are causing the issue? Useful files to check include `/proc/interrupts` and `/proc/softirqs`. Problem can be a misbehaving driver or hardware that is on it's hind legs, is about to die and it's giving out warnings.
    - One issue that's also known to cause unusual loads is actually waiting on I/O, underlying storage might be too slow for increased workload and it's showing up as load while the system is in wait state. Some of the useful utilities for this kind of troubleshooting are `vmstat`, `iotop`, `iostat`.
